__all__ = ["TextClassificationEvaluator"]
"""Metrics to assess classification of documents"""
from typing import Dict, List, Union

from sklearn.metrics import classification_report, cohen_kappa_score

from medkit.core.text import TextDocument
from medkit.text.metrics.irr_utils import krippendorff_alpha


class TextClassificationEvaluator:
    """An evaluator for attributes of TextDocuments"""

    def __init__(self, attr_label: str):
        """Initialize the text classification evaluator

        Parameters
        ----------
        attr_label:
            Label of the attribute to evaluate.
        """
        self.attr_label = attr_label

    def _extract_attr_values(
        self, docs: List[TextDocument]
    ) -> List[Union[str, int, bool]]:
        """Prepare docs attrs to compute the metric

        Parameters
        ----------
        docs : List[TextDocument]
            List of documents with attributes

        Returns
        -------
        attr_values :  List[Union[str,int,bool]]
            List with the representation of the attribute by document.
        """
        attr_values = []
        for doc in docs:
            attrs = doc.attrs.get(label=self.attr_label)

            if not attrs:
                raise ValueError(
                    f"No attribute with label {self.attr_label} was found in the"
                    " document"
                )

            attr_value = attrs[0].value
            if not isinstance(attr_value, (str, int, bool)):
                raise ValueError(
                    "The type of the attr value is not supported by this evaluator."
                    "Only str,int or bool are supported."
                )

            attr_values.append(attr_value)
        return attr_values

    def compute_classification_report(
        self,
        true_docs: List[TextDocument],
        predicted_docs: List[TextDocument],
        metrics_by_attr_value: bool = True,
        macro_average: bool = True,
    ) -> Dict[str, Union[float, int]]:
        """Compute classification metrics of document attributes giving annotated documents.
        This method uses `sklearn.metrics.classification_report` to compute
        precision, recall and F1-score for value of the attribute.

        .. warning::
            The set of true and predicted documents must be sorted to calculate the metric

        Parameters
        ----------
        true_docs:
            Text documents containing attributes of reference
        predicted_docs:
            Text documents containing predicted attributes
        metrics_by_attr_value:
            Whether return metrics by attribute value.
            If False, only average metrics are returned
        macro_average:
            Whether return the macro average. If False, the weighted mean is returned.

        Returns
        -------
        Dict[str,Union[float,int]]:
            A dictionary with the computed metrics
        """
        true_tags = self._extract_attr_values(true_docs)
        pred_tags = self._extract_attr_values(predicted_docs)

        report = classification_report(
            y_true=true_tags,
            y_pred=pred_tags,
            output_dict=True,
            zero_division=0,
        )

        prefix_avg = "macro" if macro_average else "weighted"
        scores = {
            f"{prefix_avg}_{key}": value
            for key, value in report[f"{prefix_avg} avg"].items()
        }
        scores["accuracy"] = report.pop("accuracy")

        if metrics_by_attr_value:
            for value_key in report:
                if value_key.endswith("avg"):
                    continue

                for metric_key, metric_value in report[value_key].items():
                    scores[f"{value_key}_{metric_key}"] = metric_value

        return scores

    def compute_cohen_kappa(
        self, docs_annotator_1: List[TextDocument], docs_annotator_2: List[TextDocument]
    ) -> Dict[str, Union[float, int]]:
        """Compute the cohen's kappa score, an inter-rated agreement score between two annotators.
        This method uses 'sklearn' as backend to compute the level of agreement.

        .. warning::
            The set of documents must be sorted to calculate the metric

        Parameters
        ----------
        docs_annotator_1:
            Text documents containing attributes annotated by the first annotator
        docs_annotator_2:
            Text documents to compare, these documents contain attributes
            annotated by the other annotator

        Returns
        -------
        Dict[str, Union[float, int]]:
            A dictionary with cohen's kappa score and support (number of annotated docs).
            The value is a number between -1 and 1, where 1 indicates perfect agreement; zero
            or lower indicates chance agreement.
        """
        ann1_tags = self._extract_attr_values(docs_annotator_1)
        ann2_tags = self._extract_attr_values(docs_annotator_2)

        scores = {
            "cohen_kappa": cohen_kappa_score(y1=ann1_tags, y2=ann2_tags),
            "support": len(ann1_tags),
        }

        return scores

    def compute_krippendorff_alpha(
        self, docs_annotators: List[List[TextDocument]]
    ) -> Dict[str, Union[float, int]]:
        """Compute the Krippendorff alpha score, an inter-rated agreement score between
        multiple annotators.

        .. warning::
            Documents must be sorted to calculate the metric.

        .. note::
            See :mod:`medkit.text.metrics.irr_utils.krippendorff_alpha` for more information about the score

        Parameters
        ----------
        docs_annotators:
            A list of list of Text documents containing attributes.
            The size of the list is the number of annotators to compare.

        Returns
        -------
        Dict[str, Union[float,int]]:
            A dictionary with the krippendorff alpha score, number of annotators and support (number of documents).
            A value of 1 indicates perfect reliability between annotators; zero or lower indicates
            absence of reliability.
        """
        if len(docs_annotators) < 2 or not isinstance(docs_annotators[0], list):
            raise ValueError(
                "'docs_annotators' should contain at least two list of TextDocuments to"
                " compare"
            )

        all_annotators_data = []

        for docs in docs_annotators:
            annotator_tags = self._extract_attr_values(docs)
            all_annotators_data.append(annotator_tags)
        scores = {
            "krippendorff_alpha": krippendorff_alpha(all_annotators_data),
            "nb_annotators": len(all_annotators_data),
            "support": len(all_annotators_data[0]),
        }

        return scores
